{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iphilipp/miniforge3/envs/printllama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/iphilipp/miniforge3/envs/printllama/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = \"\"\"def algorithm(n):\n",
    "    n = train_samples.shape[1]\n",
    "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
    "    return test_parity\n",
    "\"\"\"\n",
    "assistant_response = \"\"\"def algorithm(n):\n",
    "    n = train_samples.shape[1]\n",
    "    print(n)\n",
    "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
    "    return test_parity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\nYou are an expert computer science researcher and programmer, especially skilled at debugging algorithms.\\n<</SYS>>\\n\\nYou are given the following Python program:\\n```python\\ndef algorithm(n):\\n    n = train_samples.shape[1]\\n    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\\n    return test_parity\\n\\n```\\nInsert print statements in the program that will help me debug and improve the program. [/INST] def algorithm(n):\\n    n = train_samples.shape[1]\\n    print(n)\\n    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\\n    return test_parity\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messages = get_messages(initial_solution=initial_solution, assistant_response=assistant_response)\n",
    "# example = format_llama_message(*messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an expert computer science researcher and programmer, especially skilled at debugging algorithms.\n",
      "<</SYS>>\n",
      "\n",
      "You are given the following Python program:\n",
      "```python\n",
      "def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "\n",
      "```\n",
      "Insert print statements in the program that will help me debug and improve the program. [/INST] def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    print(n)\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 385, 17924, 6601, 10466, 5925, 261, 322, 27922, 29892, 7148, 2071, 24455, 472, 13490, 14009, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 3492, 526, 2183, 278, 1494, 5132, 1824, 29901, 13, 28956, 4691, 13, 1753, 5687, 29898, 29876, 1125, 13, 1678, 302, 353, 7945, 29918, 27736, 29889, 12181, 29961, 29896, 29962, 13, 1678, 11307, 29918, 5344, 353, 7442, 29889, 29882, 1429, 3552, 14968, 29918, 27736, 29892, 7945, 29918, 862, 537, 29889, 690, 14443, 29898, 29896, 29892, 448, 29896, 4961, 13, 1678, 736, 1243, 29918, 862, 537, 13, 13, 28956, 13, 17491, 1596, 9506, 297, 278, 1824, 393, 674, 1371, 592, 4744, 322, 11157, 278, 1824, 29889, 518, 29914, 25580, 29962, 822, 5687, 29898, 29876, 1125, 13, 1678, 302, 353, 7945, 29918, 27736, 29889, 12181, 29961, 29896, 29962, 13, 1678, 1596, 29898, 29876, 29897, 13, 1678, 11307, 29918, 5344, 353, 7442, 29889, 29882, 1429, 3552, 14968, 29918, 27736, 29892, 7945, 29918, 862, 537, 29889, 690, 14443, 29898, 29896, 29892, 448, 29896, 4961, 13, 1678, 736, 1243, 29918, 862, 537, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 385, 17924, 6601, 10466, 5925, 261, 322, 27922, 29892, 7148, 2071, 24455, 472, 13490, 14009, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 3492, 526, 2183, 278, 1494, 5132, 1824, 29901, 13, 28956, 4691, 13, 1753, 5687, 29898, 29876, 1125, 13, 1678, 302, 353, 7945, 29918, 27736, 29889, 12181, 29961, 29896, 29962, 13, 1678, 11307, 29918, 5344, 353, 7442, 29889, 29882, 1429, 3552, 14968, 29918, 27736, 29892, 7945, 29918, 862, 537, 29889, 690, 14443, 29898, 29896, 29892, 448, 29896, 4961, 13, 1678, 736, 1243, 29918, 862, 537, 13, 13, 28956, 13, 17491, 1596, 9506, 297, 278, 1824, 393, 674, 1371, 592, 4744, 322, 11157, 278, 1824, 29889, 518, 29914, 25580, 29962, 822, 5687, 29898, 29876, 1125, 13, 1678, 302, 353, 7945, 29918, 27736, 29889, 12181, 29961, 29896, 29962, 13, 1678, 1596, 29898, 29876, 29897, 13, 1678, 11307, 29918, 5344, 353, 7442, 29889, 29882, 1429, 3552, 14968, 29918, 27736, 29892, 7945, 29918, 862, 537, 29889, 690, 14443, 29898, 29896, 29892, 448, 29896, 4961, 13, 1678, 736, 1243, 29918, 862, 537, 13, 2]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"printdata.json\")\n",
    "# dataset\n",
    "\n",
    "\n",
    "example = dataset[\"train\"][0][\"text\"]\n",
    "\n",
    "print(example)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "tok = tokenize(example, tokenizer)\n",
    "tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are an expert computer science researcher and programmer, especially skilled at debugging algorithms.\n",
      "<</SYS>>\n",
      "\n",
      "You are given the following Python program:\n",
      "```python\n",
      "def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "\n",
      "```\n",
      "Insert print statements in the program that will help me debug and improve the program. [/INST] def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    print(n)\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "</s>\n",
      "<s> [INST] <<SYS>>\n",
      "You are an expert computer science researcher and programmer, especially skilled at debugging algorithms.\n",
      "<</SYS>>\n",
      "\n",
      "You are given the following Python program:\n",
      "```python\n",
      "def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "\n",
      "```\n",
      "Insert print statements in the program that will help me debug and improve the program. [/INST] def algorithm(n):\n",
      "    n = train_samples.shape[1]\n",
      "    print(n)\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tok[\"input_ids\"]))\n",
    "print(tokenizer.decode(tok[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "subset_dataset = dataset.select(range(5))\n",
    "train_dataset = subset_dataset.select(range(1))\n",
    "eval_dataset = subset_dataset.select(range(1))\n",
    "\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "\n",
    "# add eos token\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "#Â tokenize\n",
    "tokenized_train_dataset = train_dataset.map(lambda data_point: generate_and_tokenize_prompt(data_point, tokenizer))\n",
    "tokenized_val_dataset = eval_dataset.map(lambda data_point: generate_and_tokenize_prompt(data_point, tokenizer))\n",
    "\n",
    "# torchrun --nproc_per_node=1 finetune.py model.codellama_7b\n",
    "# python finetune.py --config-name=model/codellama_7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer SELECT COUNT(*) FROM head WHERE age > 56\n",
      "context CREATE TABLE head (age INTEGER)\n",
      "question How many heads of the departments are older than 56 ?\n",
      "input_ids [1, 887, 526, 263, 13988, 1426, 29899, 517, 29899, 4176, 1904, 29889, 3575, 4982, 338, 304, 1234, 5155, 1048, 263, 2566, 29889, 887, 526, 2183, 263, 1139, 322, 3030, 11211, 697, 470, 901, 6131, 29889, 13, 13, 3492, 1818, 1962, 278, 3758, 2346, 393, 6089, 278, 1139, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 5328, 1784, 15883, 310, 278, 5840, 1860, 526, 9642, 1135, 29871, 29945, 29953, 1577, 13, 13, 2277, 29937, 15228, 29901, 13, 27045, 10911, 2343, 313, 482, 2672, 4330, 17070, 29897, 13, 13, 2277, 29937, 13291, 29901, 13, 6404, 21122, 22798, 3895, 2343, 5754, 5046, 1405, 29871, 29945, 29953, 13, 2]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [1, 887, 526, 263, 13988, 1426, 29899, 517, 29899, 4176, 1904, 29889, 3575, 4982, 338, 304, 1234, 5155, 1048, 263, 2566, 29889, 887, 526, 2183, 263, 1139, 322, 3030, 11211, 697, 470, 901, 6131, 29889, 13, 13, 3492, 1818, 1962, 278, 3758, 2346, 393, 6089, 278, 1139, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 5328, 1784, 15883, 310, 278, 5840, 1860, 526, 9642, 1135, 29871, 29945, 29953, 1577, 13, 13, 2277, 29937, 15228, 29901, 13, 27045, 10911, 2343, 313, 482, 2672, 4330, 17070, 29897, 13, 13, 2277, 29937, 13291, 29901, 13, 6404, 21122, 22798, 3895, 2343, 5754, 5046, 1405, 29871, 29945, 29953, 13, 2]\n"
     ]
    }
   ],
   "source": [
    "for feature in tokenized_train_dataset.features:\n",
    "    print(feature, tokenized_train_dataset[0][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
      "\n",
      "You must output the SQL query that answers the question.\n",
      "\n",
      "### Input:\n",
      "How many heads of the departments are older than 56 ?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE head (age INTEGER)\n",
      "\n",
      "### Response:\n",
      "SELECT COUNT(*) FROM head WHERE age > 56\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
      "\n",
      "You must output the SQL query that answers the question.\n",
      "\n",
      "### Input:\n",
      "How many heads of the departments are older than 56 ?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE head (age INTEGER)\n",
      "\n",
      "### Response:\n",
      "SELECT COUNT(*) FROM head WHERE age > 56\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import json\n",
    "import copy\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.pad_token = DEFAULT_PAD_TOKEN\n",
    "\n",
    "def _tokenize_fn(strings, tokenizer):\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=2000,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    targets,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = -100\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n",
    "\n",
    "\n",
    "list_data_dict = jload(\"data/debug_data.json\")\n",
    "\n",
    "\n",
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "sources = [\n",
    "    prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "    for example in list_data_dict[:10]\n",
    "]\n",
    "targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "\n",
    "data_dict = preprocess(sources, targets, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain why the following fraction is equivalent to 1/4\n",
      "\n",
      "### Input:\n",
      "4/16\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(sources[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.</s>\n"
     ]
    }
   ],
   "source": [
    "print(targets[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain why the following fraction is equivalent to 1/4\n",
      "\n",
      "### Input:\n",
      "4/16\n",
      "\n",
      "### Response:The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data_dict[\"input_ids\"][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = \"\"\"def algorithm(train_samples, train_parity, test_samples):\n",
    "    n = train_samples.shape[1]\n",
    "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
    "    return test_parity\"\"\"\n",
    "\n",
    "human_message = human_message.format(solution=solution)\n",
    "assistant_message = assistant_message.format(solution=solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are an expert computer science researcher and programmer, especially skilled at debugging algorithms.\n",
      "<</SYS>>\n",
      "\n",
      "You are given the following Python program:\n",
      "\n",
      "```python\n",
      "def algorithm(train_samples, train_parity, test_samples):\n",
      "    n = train_samples.shape[1]\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity\n",
      "```\n",
      "Insert print statements in the program that will help me debug and improve the program. [/INST] def algorithm(train_samples, train_parity, test_samples):\n",
      "    n = train_samples.shape[1]\n",
      "    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))\n",
      "    return test_parity </s>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
